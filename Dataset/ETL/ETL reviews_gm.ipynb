{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from scipy.stats import ttest_ind # type: ignore\n",
    "from sklearn.feature_extraction.text import CountVectorizer # type: ignore\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # type: ignore\n",
    "from textblob import TextBlob # type: ignore\n",
    "from wordcloud import WordCloud # type: ignore\n",
    "import glob # type: ignore\n",
    "import os # type: ignore\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la ruta de la carpeta\n",
    "folder_path = os.path.join(\"Google Maps\", \"reviews-estados\", \"review-Florida\")\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada archivo\n",
    "df_list = []\n",
    "\n",
    "# Iterar sobre los archivos del 1 al 19\n",
    "for i in range(1, 20):  # del 1 al 19\n",
    "    file_path = os.path.join(folder_path, f\"{i}.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):  # Verificar si el archivo existe\n",
    "        try:\n",
    "            df_temp = pd.read_json(file_path, lines=True)  # Cargar el JSON lÃ­nea por lÃ­nea\n",
    "            df_list.append(df_temp)  # Agregar al listado de DataFrames\n",
    "        except ValueError as e:\n",
    "            print(f\"Error al procesar {file_path}: {e}\")\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "reviews_gm = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap_id = pd.read_csv('metadatos_fl_final.csv', usecols= ['id','gmap_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar reviews_gm para que solo contenga las filas con gmap_id presentes en gmap_id\n",
    "filtered_reviews_gm = reviews_gm[reviews_gm['gmap_id'].isin(gmap_id['gmap_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_gm = filtered_reviews_gm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corregir el formato de time (convertir de milisegundos a segundos)\n",
    "reviews_gm[\"time\"] = reviews_gm[\"time\"] // 1000  # DivisiÃ³n entera para evitar decimales\n",
    "\n",
    "# Convertir 'time' de timestamp UNIX a datetime\n",
    "reviews_gm[\"date\"] = pd.to_datetime(reviews_gm[\"time\"], unit=\"s\")\n",
    "\n",
    "# Cambiar el formato\n",
    "reviews_gm['date'] = pd.to_datetime(reviews_gm['date']).dt.strftime('%Y-%m-%d')\n",
    "reviews_gm['date'] = pd.to_datetime(reviews_gm['date'])  # Mantiene datetime sin la hora\n",
    "\n",
    "reviews_gm = reviews_gm.drop(columns=\"time\")\n",
    "\n",
    "# Definir el rango de fechas permitido\n",
    "end_date = pd.Timestamp(\"2022-01-19\")\n",
    "start_date = end_date - pd.DateOffset(years=5)  # Calcula la fecha mÃ­nima automÃ¡ticamente\n",
    "\n",
    "# Filtrar solo los registros dentro del rango de los Ãºltimos 5 aÃ±os\n",
    "reviews_gm = reviews_gm[(reviews_gm[\"date\"] >= start_date) & (reviews_gm[\"date\"] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas 'pics' y 'resp' porque tienen muchos nulos\n",
    "reviews = reviews_gm.drop(columns=[\"pics\", \"resp\"])\n",
    "\n",
    "# Eliminar las columnas 'name' porque no es util para los anÃ¡lisis\n",
    "reviews = reviews.drop(columns=[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el tamaÃ±o del chunk\n",
    "chunk_size = 50000  # Procesar en bloques de 50000 reviews\n",
    "\n",
    "# FunciÃ³n de anÃ¡lisis de sentimiento\n",
    "def analyze_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vader_score = analyzer.polarity_scores(text)['compound']\n",
    "    textblob_score = TextBlob(text).sentiment.polarity\n",
    "    return pd.Series([vader_score, textblob_score])\n",
    "\n",
    "# Iterar sobre los datos en chunks\n",
    "for start in range(0, len(reviews), chunk_size):\n",
    "    end = min(start + chunk_size, len(reviews))\n",
    "    chunk = reviews.iloc[start:end].copy()  # Copia para evitar modificaciones en el DataFrame original\n",
    "\n",
    "    # Aplicar anÃ¡lisis de sentimiento con swifter\n",
    "    chunk[['vader_score', 'textblob_score']] = chunk['text'].swifter.apply(lambda x: analyze_sentiment(str(x))).apply(pd.Series)\n",
    "\n",
    "    # ğŸ”¹ Guardar cada chunk en un archivo Parquet para no perder datos\n",
    "    chunk.to_parquet(f\"sentiment_chunk_gm_{start}.parquet\")\n",
    "\n",
    "    print(f\"âœ… Procesado y guardado chunk {start}-{end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todos los archivos guardados\n",
    "chunk_files = glob.glob(\"sentiment_chunk_gm_*.parquet\")\n",
    "\n",
    "# Cargar y combinar todos los chunks\n",
    "reviews_final = pd.concat([pd.read_parquet(f) for f in chunk_files], ignore_index=True)\n",
    "\n",
    "# Verificar que todo estÃ© unido\n",
    "print(f\"Dataset final: {reviews_final.shape[0]} filas, {reviews_final.shape[1]} columnas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap_id.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_final['has_text'] = reviews_final['text'].notnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_final = reviews_final.drop(columns=['text', 'user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir reviews_final con el DataFrame de gmap_id para agregar la columna 'id'\n",
    "reviews_final = reviews_final.merge(gmap_id, on=\"gmap_id\", how=\"left\")\n",
    "\n",
    "# Verificar la estructura despuÃ©s del merge\n",
    "print(reviews_final.info())\n",
    "print(reviews_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el orden lÃ³gico de las columnas\n",
    "column_order = ['id','gmap_id', 'rating', 'has_text', 'vader_score', 'textblob_score','date']\n",
    "\n",
    "# Aplicar el orden de columnas asegurando que no haya errores por columnas faltantes\n",
    "reviews_final = reviews_final[column_order]\n",
    "\n",
    "# Reiniciar el Ã­ndice despuÃ©s de todas las transformaciones\n",
    "reviews_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_final.to_csv('reviews_gm.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
