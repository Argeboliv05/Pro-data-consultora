{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ETL `metadata_gm`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd # type: ignore\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la ruta de la carpeta\n",
    "folder_path = os.path.join(\"Google Maps\", \"metadata-sitios\")\n",
    "\n",
    "# Lista para almacenar los DataFrames de cada archivo\n",
    "df_list = []\n",
    "\n",
    "# Iterar sobre los archivos del 1 al 11\n",
    "for i in range(1, 12):  # del 1 al 11\n",
    "    file_path = os.path.join(folder_path, f\"{i}.json\")\n",
    "    \n",
    "    if os.path.exists(file_path):  # Verificar si el archivo existe\n",
    "        try:\n",
    "            df_temp = pd.read_json(file_path, lines=True)  # Cargar el JSON línea por línea\n",
    "            df_list.append(df_temp)  # Agregar al listado de DataFrames\n",
    "        except ValueError as e:\n",
    "            print(f\"Error al procesar {file_path}: {e}\")\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "metadatos_gm = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadatos_gm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que la columna 'category' contiene listas (si está en formato string, convertirla)\n",
    "metadata['category'] = metadata['category'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Aplanar la lista de categorías y contar frecuencia\n",
    "category_counts = Counter(itertools.chain.from_iterable(metadata['category'].dropna()))\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_categories = pd.DataFrame(category_counts.items(), columns=['Category', 'Frequency'])\n",
    "\n",
    "# Filtrar categorías que contengan \"restaurant\" o \"food\" (sin importar mayúsculas/minúsculas)\n",
    "filtered_categories = df_categories[\n",
    "    df_categories[\"Category\"].str.contains(r\"restaurant|food|\\bbar\\b\", case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las categorías permitidas en un conjunto en minúsculas\n",
    "allowed_categories = set(filtered_categories[\"Category\"].str.lower().str.strip())\n",
    "\n",
    "# Separar la columna 'category' en filas individuales (una categoría por fila)\n",
    "metadata_exploded = metadata.explode(\"category\")\n",
    "\n",
    "# Convertir a minúsculas y quitar espacios extra para evitar problemas\n",
    "metadata_exploded[\"category\"] = metadata_exploded[\"category\"].str.lower().str.strip()\n",
    "\n",
    "# Filtrar solo las filas donde la categoría está en allowed_categories\n",
    "metadata_filtered = metadata_exploded[metadata_exploded[\"category\"].isin(allowed_categories)]\n",
    "\n",
    "# Volver a agrupar por gmap_id para restaurar la estructura original\n",
    "metadata_filtered = metadata_filtered.groupby(\"gmap_id\").first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 264449 entries, 0 to 264448\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   gmap_id           264449 non-null  object \n",
      " 1   name              264449 non-null  object \n",
      " 2   address           262834 non-null  object \n",
      " 3   description       81601 non-null   object \n",
      " 4   latitude          264449 non-null  float64\n",
      " 5   longitude         264449 non-null  float64\n",
      " 6   category          264449 non-null  object \n",
      " 7   avg_rating        264449 non-null  float64\n",
      " 8   num_of_reviews    264449 non-null  int64  \n",
      " 9   price             118326 non-null  object \n",
      " 10  hours             234272 non-null  object \n",
      " 11  MISC              259331 non-null  object \n",
      " 12  state             235441 non-null  object \n",
      " 13  relative_results  217914 non-null  object \n",
      " 14  url               264449 non-null  object \n",
      "dtypes: float64(3), int64(1), object(11)\n",
      "memory usage: 30.3+ MB\n"
     ]
    }
   ],
   "source": [
    "metadata_filtered.info()\n",
    "\n",
    "metadata = metadata_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de estados de EE.UU.\n",
    "state_abbreviations = {\n",
    "    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\", \"Florida\": \"FL\", \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\", \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\", \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\", \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\", \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\", \"New York\": \"NY\", \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\", \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\", \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\", \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\"\n",
    "}\n",
    "\n",
    "# Función para separar la columna address\n",
    "def split_address(df):\n",
    "    # Eliminar direcciones nulas\n",
    "    df = df.dropna(subset=['address']).copy()\n",
    "    \n",
    "    # Contar la cantidad de comas en cada dirección\n",
    "    comma_counts = df['address'].str.count(',')\n",
    "    \n",
    "    # Casos con exactamente 3 comas (estructura correcta de 4 secciones)\n",
    "    valid_rows = comma_counts == 3\n",
    "    split_data = df.loc[valid_rows, 'address'].str.split(',', expand=True)\n",
    "    df.loc[valid_rows, 'street_address'] = split_data[1].str.strip()\n",
    "    df.loc[valid_rows, 'city'] = split_data[2].str.strip()\n",
    "    state_cp = split_data[3].str.strip().str.split(' ', n=1, expand=True)\n",
    "    df.loc[valid_rows, 'states'] = state_cp[0].copy()\n",
    "    df.loc[valid_rows, 'CP'] = state_cp[1].copy()\n",
    "    \n",
    "    # Casos con más de 3 comas (nombre con comas)\n",
    "    complex_rows = comma_counts > 3\n",
    "    split_data = df.loc[complex_rows, 'address'].str.rsplit(',', n=3, expand=True)\n",
    "    df.loc[complex_rows, 'street_address'] = split_data[1].str.strip()\n",
    "    df.loc[complex_rows, 'city'] = split_data[2].str.strip()\n",
    "    state_cp = split_data[3].str.strip().str.split(' ', n=1, expand=True)\n",
    "    df.loc[complex_rows, 'states'] = state_cp[0].copy()\n",
    "    df.loc[complex_rows, 'CP'] = state_cp[1].copy()\n",
    "    \n",
    "    # Casos con exactamente 2 comas (sin dirección, pero con ciudad y estado/CP)\n",
    "    partial_rows = comma_counts == 2\n",
    "    split_data = df.loc[partial_rows, 'address'].str.split(',', expand=True)\n",
    "    df.loc[partial_rows, 'city'] = split_data[1].str.strip()\n",
    "    state_cp = split_data[2].str.strip().str.split(' ', n=1, expand=True)\n",
    "    df.loc[partial_rows, 'states'] = state_cp[0].copy()\n",
    "    df.loc[partial_rows, 'CP'] = state_cp[1].copy()\n",
    "    \n",
    "    # Casos con solo 1 coma (nombre + estado completo con o sin CP)\n",
    "    state_only_rows = comma_counts == 1\n",
    "    split_data = df.loc[state_only_rows, 'address'].str.split(',', expand=True)\n",
    "    df.loc[state_only_rows, 'states'] = split_data[1].str.strip().str.extract(f\"({'|'.join(state_abbreviations.keys())})\")[0]\n",
    "    df.loc[state_only_rows, 'states'] = df.loc[state_only_rows, 'states'].map(state_abbreviations)\n",
    "    df.loc[state_only_rows, 'CP'] = split_data[1].str.extract(r'(\\d{5})')[0]\n",
    "    \n",
    "    # Eliminar registros con solo una sección en 'address', contienen muchos nulos en sus campos\n",
    "    df = df[comma_counts > 0]\n",
    "    \n",
    "    # Identificar registros que no encajan en ninguna de las estructuras tratadas\n",
    "    unprocessed_rows = ~valid_rows & ~complex_rows & ~partial_rows & ~state_only_rows\n",
    "    unprocessed_data = df.loc[unprocessed_rows, ['name', 'address']]\n",
    "    \n",
    "    return df, unprocessed_data\n",
    "\n",
    "# Aplicar la función\n",
    "metadata, unprocessed_metadata = split_address(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar registros donde el estado es Florida\n",
    "metadata = metadata[metadata['states'] == 'FL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15794 entries, 1 to 263843\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   gmap_id           15794 non-null  object \n",
      " 1   name              15794 non-null  object \n",
      " 2   address           15794 non-null  object \n",
      " 3   description       4521 non-null   object \n",
      " 4   latitude          15794 non-null  float64\n",
      " 5   longitude         15794 non-null  float64\n",
      " 6   category          15794 non-null  object \n",
      " 7   avg_rating        15794 non-null  float64\n",
      " 8   num_of_reviews    15794 non-null  int64  \n",
      " 9   price             5881 non-null   object \n",
      " 10  hours             13768 non-null  object \n",
      " 11  MISC              15507 non-null  object \n",
      " 12  state             13818 non-null  object \n",
      " 13  relative_results  12078 non-null  object \n",
      " 14  url               15794 non-null  object \n",
      " 15  street_address    15578 non-null  object \n",
      " 16  city              15793 non-null  object \n",
      " 17  states            15794 non-null  object \n",
      " 18  CP                15794 non-null  object \n",
      "dtypes: float64(3), int64(1), object(15)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "metadata_fl = metadata.copy()\n",
    "\n",
    "metadata_fl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la información de `state`, analizamos si el negocio se encuentra abierto o no, y creamos una la columna `is_open`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fl = metadata_fl.copy()  # Evitar problemas de vistas sobre el DataFrame\n",
    "metadata_fl['is_open'] = metadata_fl['state'].apply(lambda x: 0 if x == 'Permanently closed' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan todas las columnas que no se van a usar en el análisis y las que ya fueron analizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fl = metadata_fl.drop(columns = ['address', \n",
    "                                          'description', \n",
    "                                          'url', \n",
    "                                          'relative_results',\n",
    "                                          'state',\n",
    "                                          'hours',\n",
    "                                          'price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombrar columnas para que coincidan con la tabla de `yelp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fl = metadata_fl.rename(columns={'avg_rating': 'stars', \n",
    "                                          'num_of_reviews': 'review_count', \n",
    "                                          'states': 'state', \n",
    "                                          'CP' : 'postal_code',\n",
    "                                          'category': 'categories'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación de la columna MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar valores nulos en MISC con un diccionario vacío\n",
    "metadata_fl['MISC'] = metadata_fl['MISC'].fillna('{}')\n",
    "\n",
    "# Convertir la columna MISC a diccionarios reales si es un string en formato JSON\n",
    "metadata_fl['MISC'] = metadata_fl['MISC'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Expandir los diccionarios en columnas\n",
    "misc_expanded = metadata_fl['MISC'].apply(pd.Series)\n",
    "\n",
    "# Unir las nuevas columnas al DataFrame original\n",
    "metadata_fl = pd.concat([metadata_fl, misc_expanded], axis=1)\n",
    "\n",
    "# Eliminar la columna original MISC\n",
    "metadata_fl = metadata_fl.drop(columns=['MISC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `Health & safety` describe las medidas de precaucion y higiene que se tomaron durante la pandemia, por lo que no es información que utilicemos en este proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar columna  'Health & safety'\n",
    "metadata_fl = metadata_fl.drop(columns = 'Health & safety')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis y expansión de `Service options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer todas las categorías únicas dentro de \"Service options\"\n",
    "unique_service_options = set()\n",
    "\n",
    "for value in metadata_fl[\"Service options\"].dropna():  # Ignorar valores nulos\n",
    "    if isinstance(value, list):  # Confirmar que es una lista\n",
    "        clean_values = [x.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for x in value]\n",
    "        unique_service_options.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "\n",
    "# Crear DataFrame con las opciones en formato binario\n",
    "service_options_df = pd.DataFrame()\n",
    "\n",
    "for option in unique_service_options:\n",
    "    service_options_df[option] = metadata_fl[\"Service options\"].apply(\n",
    "        lambda x: 1 if isinstance(x, list) and option in [i.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for i in x] else 0\n",
    "    )\n",
    "\n",
    "# Sumar la cantidad de negocios que tienen cada opción activada\n",
    "option_counts = service_options_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionar categorías en 'delivery'\n",
    "metadata_fl[\"delivery\"] = metadata_fl[\"Service options\"].apply(\n",
    "    lambda x: 1 if isinstance(x, list) and any(opt in x for opt in [\"Delivery\", \"Same-day delivery\", \"No-contact delivery\"]) else 0\n",
    ")\n",
    "\n",
    "# Fusionar categorías en 'takeout'\n",
    "metadata_fl[\"takeout\"] = metadata_fl[\"Service options\"].apply(\n",
    "    lambda x: 1 if isinstance(x, list) and any(opt in x for opt in [\"Takeout\", \"Curbside pickup\"]) else 0\n",
    ")\n",
    "\n",
    "# Crear columnas individuales sin modificar\n",
    "metadata_fl[\"dinein\"] = metadata_fl[\"Service options\"].apply(lambda x: 1 if isinstance(x, list) and \"Dine-in\" in x else 0)\n",
    "metadata_fl[\"outdoor_seating\"] = metadata_fl[\"Service options\"].apply(lambda x: 1 if isinstance(x, list) and \"Outdoor seating\" in x else 0)\n",
    "metadata_fl[\"drivethrough\"] = metadata_fl[\"Service options\"].apply(lambda x: 1 if isinstance(x, list) and \"Drive-through\" in x else 0)\n",
    "\n",
    "# Eliminar la columna original \"Service options\"\n",
    "metadata_fl = metadata_fl.drop(columns=[\"Service options\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos la columna `Popular for`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer todas las categorías únicas dentro de \"Popular for\"\n",
    "unique_popular_for = set()\n",
    "\n",
    "for value in metadata_fl[\"Popular for\"].dropna():  # Ignorar valores nulos\n",
    "    if isinstance(value, list):  # Confirmar que es una lista\n",
    "        clean_values = [x.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for x in value]\n",
    "        unique_popular_for.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "\n",
    "# Crear DataFrame con las opciones en formato binario\n",
    "popular_for_df = pd.DataFrame()\n",
    "\n",
    "for option in unique_popular_for:\n",
    "    popular_for_df[option] = metadata_fl[\"Popular for\"].apply(\n",
    "        lambda x: 1 if isinstance(x, list) and option in [i.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for i in x] else 0\n",
    "    )\n",
    "\n",
    "# Sumar la cantidad de negocios que tienen cada opción activada\n",
    "option_counts = popular_for_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lunch', 'solo_dining', 'dinner', 'good_for_working_on_laptop', 'breakfast'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_popular_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar los valores en \"Popular for\"\n",
    "def clean_popular_for(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            value = ast.literal_eval(value)  # Convertir string a lista\n",
    "        except (ValueError, SyntaxError):\n",
    "            return \"\"  # Si falla la conversión, devolver vacío\n",
    "    \n",
    "    if isinstance(value, list):\n",
    "        clean_values = [x.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for x in value]\n",
    "        return \", \".join(sorted(set(clean_values)))  # Eliminar duplicados y unir\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "# Aplicar limpieza\n",
    "metadata_fl[\"Popular for\"] = metadata_fl[\"Popular for\"].apply(clean_popular_for)\n",
    "\n",
    "# Expansión en variables dummies\n",
    "popular_for_dummies = metadata_fl[\"Popular for\"].str.get_dummies(sep=\", \")\n",
    "\n",
    "# Renombrar columnas para mayor claridad\n",
    "popular_for_dummies.columns = [f\"Popular_for_{col}\" for col in popular_for_dummies.columns]\n",
    "\n",
    "# Unir al dataframe original y eliminar la columna original\n",
    "metadata_fl = pd.concat([metadata_fl, popular_for_dummies], axis=1).drop(columns=[\"Popular for\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos la columna `Accessibility`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Accessibility\"\n",
    "def extract_unique_accessibility(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_accessibility = extract_unique_accessibility(metadata_fl[\"Accessibility\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Accessibility' en 'wheelchair_friendly'\n",
    "def transform_wheelchair_friendly(df, column):\n",
    "    wheelchair_categories = {\n",
    "        'wheelchair_rental',\n",
    "        'wheelchair_accessible_restroom',\n",
    "        'wheelchair_accessible_elevator',\n",
    "        'wheelchair_accessible_entrance',\n",
    "        'wheelchair_accessible_parking_lot',\n",
    "        'wheelchair_accessible_seating'\n",
    "    }\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def check_wheelchair_friendly(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return 0\n",
    "        if isinstance(value, list):\n",
    "            normalized_values = {normalize(item) for item in value}\n",
    "            return 1 if wheelchair_categories & normalized_values else 0\n",
    "        return 0\n",
    "    \n",
    "    df['wheelchair_friendly'] = df[column].apply(check_wheelchair_friendly)\n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_wheelchair_friendly(metadata_fl, 'Accessibility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis y expansión de `Offerings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Offerings\"\n",
    "def extract_unique_offerings(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_offerings = extract_unique_offerings(metadata_fl[\"Offerings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Offerings'\n",
    "def transform_offerings(df, column):\n",
    "    remove_categories = {\n",
    "        'buys_used_goods', 'car_wash', 'full_service_gas', 'ethanolfree_gas', 'check_cashing', 'oil_change', \n",
    "        'food', 'repair_services', 'service_guarantee', 'assembly_service', 'prepared_foods', 'food_at_bar', \n",
    "        'small_plates', 'latenight_food'\n",
    "    }\n",
    "    alcohol_beverage = {'hard_liquor', 'alcohol', 'beer', 'cocktails', 'wine', 'happy_hour_drinks'}\n",
    "    healthy_food = {'salad_bar', 'organic_dishes', 'healthy_options', 'halal_food', 'vegetarian_options', 'organic_products'}\n",
    "    kids = {\"kids'_menu\", \"kids'_toys\"}\n",
    "    fast_comfort_food = {'comfort_food', 'happy_hour_food', 'quick_bite'}\n",
    "    separate_categories = {'coffee', 'dancing', 'all_you_can_eat', 'braille_menu'}\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def categorize_offerings(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, (list, np.ndarray, pd.Series)):\n",
    "                normalized_values = {normalize(item) for item in value}  # Normalizar antes de comparar\n",
    "                return normalized_values - remove_categories\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(categorize_offerings)\n",
    "    \n",
    "    df['alcohol_beverage'] = df[column].apply(lambda x: 1 if alcohol_beverage & x else 0)\n",
    "    df['healthy_food'] = df[column].apply(lambda x: 1 if healthy_food & x else 0)\n",
    "    df['kids'] = df[column].apply(lambda x: 1 if kids & x else 0)\n",
    "    df['fast_comfort_food'] = df[column].apply(lambda x: 1 if fast_comfort_food & x else 0)\n",
    "    \n",
    "    for category in separate_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_offerings(metadata_fl, 'Offerings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos `Dining options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         None\n",
       "9         None\n",
       "17        None\n",
       "18        None\n",
       "19        None\n",
       "          ... \n",
       "191048    None\n",
       "191051    None\n",
       "204241    None\n",
       "204271    None\n",
       "245363    None\n",
       "Name: Dining options, Length: 7617, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un conjunto vacío para almacenar las categorías únicas\n",
    "dining_options_set = set()\n",
    "\n",
    "# Recorrer la columna y extraer las categorías\n",
    "metadata_fl[\"Dining options\"].dropna().apply(lambda x: dining_options_set.update(x) if isinstance(x, list) else dining_options_set.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para expandir 'Dining options' en columnas binarias\n",
    "def expand_dining_options(df, column):\n",
    "    unique_categories = {\n",
    "        'Catering', 'Outside food allowed', 'Breakfast', 'Seating', 'Pay ahead', \n",
    "        'Lunch', 'Dinner', 'Dessert', 'Counter service'\n",
    "    }\n",
    "    \n",
    "    def parse_dining_options(value):\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, list):\n",
    "                return set(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(parse_dining_options)\n",
    "    \n",
    "    for category in unique_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Función para fusionar 'Popular for' y 'Dining options'\n",
    "def merge_popular_dining(df):\n",
    "    # Crear nuevas columnas fusionadas\n",
    "    df['breakfast'] = df[['Popular_for_breakfast', 'Breakfast']].max(axis=1)\n",
    "    df['lunch'] = df[['Popular_for_lunch', 'Lunch']].max(axis=1)\n",
    "    df['dinner'] = df[['Popular_for_dinner', 'Dinner']].max(axis=1)\n",
    "    df['dessert'] = df['Dessert']  # Mantener Dessert tal cual\n",
    "    \n",
    "    # Mantener sin cambios las otras categorías\n",
    "    keep_columns = [\n",
    "        'Popular_for_solo_dining', 'Popular_for_good_for_working_on_laptop',\n",
    "        'Seating', 'Pay ahead', 'Outside food allowed', 'Catering', 'Counter service'\n",
    "    ]\n",
    "    \n",
    "    # Eliminar columnas originales\n",
    "    df.drop(columns=['Popular_for_breakfast', 'Popular_for_lunch', 'Popular_for_dinner', 'Breakfast', 'Lunch', 'Dinner'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar la expansión y fusión\n",
    "metadata_fl = expand_dining_options(metadata_fl, 'Dining options')\n",
    "metadata_fl = merge_popular_dining(metadata_fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis y expansión de `Atmosphere`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expansión de la columna \"Atmosphere\"\n",
    "def expand_atmosphere(df, column):\n",
    "    df[column] = df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    atmosphere_list = set()\n",
    "    df[column].dropna().apply(lambda x: atmosphere_list.update(x))\n",
    "\n",
    "    for category in atmosphere_list:\n",
    "        df[category.lower().replace(\" \", \"_\")] = df[column].apply(lambda x: 1 if isinstance(x, list) and category in x else 0)\n",
    "\n",
    "# Aplicar la transformación a \"Atmosphere\"\n",
    "expand_atmosphere(metadata_fl, \"Atmosphere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Atmosphere' en categorías agrupadas\n",
    "def transform_atmosphere(df, column):\n",
    "    casual_categories = {'Casual', 'Cozy', 'Quiet'}\n",
    "    formal_categories = {'Upscale', 'Historic'}\n",
    "    trendy_categories = {'Trending'}\n",
    "    romantic_categories = {'Romantic'}\n",
    "    \n",
    "    def parse_atmosphere(value):\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, list):\n",
    "                return set(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(parse_atmosphere)\n",
    "    \n",
    "    df['casual'] = df[column].apply(lambda x: 1 if casual_categories & x else 0)\n",
    "    df['formal'] = df[column].apply(lambda x: 1 if formal_categories & x else 0)\n",
    "    df['trendy'] = df[column].apply(lambda x: 1 if trendy_categories & x else 0)\n",
    "    df['romantic'] = df[column].apply(lambda x: 1 if romantic_categories & x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar la transformación\n",
    "metadata_fl = transform_atmosphere(metadata_fl, 'Atmosphere')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis y expansión de `Planning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4         None\n",
       "11519     None\n",
       "26691     None\n",
       "70232     None\n",
       "105491    None\n",
       "          ... \n",
       "191052    None\n",
       "204241    None\n",
       "213813    None\n",
       "221977    None\n",
       "263843    None\n",
       "Name: Planning, Length: 3051, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraer y listar las categorías únicas\n",
    "planning_list = set()\n",
    "\n",
    "metadata_fl[\"Planning\"].dropna().apply(lambda x: planning_list.update(ast.literal_eval(x) if isinstance(x, str) else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Planning' en categorías agrupadas\n",
    "def transform_planning(df, column):\n",
    "    reservation_categories = {\n",
    "        'Brunch reservations recommended', 'Appointments recommended', 'Accepts reservations',\n",
    "        'Dinner reservations recommended', 'Lunch reservations recommended'\n",
    "    }\n",
    "    wait_categories = {'Usually a wait'}\n",
    "    quick_visit_categories = {'Quick visit'}\n",
    "    \n",
    "    def parse_planning(value):\n",
    "        if isinstance(value, float) and pd.isna(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, list):\n",
    "                return set(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(parse_planning)\n",
    "    \n",
    "    df['with_reservation'] = df[column].apply(lambda x: 1 if reservation_categories & x else 0)\n",
    "    df['usually_a_wait'] = df[column].apply(lambda x: 1 if wait_categories & x else 0)\n",
    "    df['quick_visit'] = df[column].apply(lambda x: 1 if quick_visit_categories & x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar la transformación\n",
    "metadata_fl = transform_planning(metadata_fl, 'Planning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos `From the business`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9         None\n",
       "171803    None\n",
       "171898    None\n",
       "171900    None\n",
       "171944    None\n",
       "          ... \n",
       "190647    None\n",
       "190677    None\n",
       "190966    None\n",
       "190985    None\n",
       "191013    None\n",
       "Name: From the business, Length: 702, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_list = set()\n",
    "\n",
    "# Iterar sobre las listas dentro de la columna\n",
    "metadata_fl[\"From the business\"].dropna().apply(lambda x: business_list.update(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columnas binarias\n",
    "metadata_fl[\"black_owned\"] = metadata_fl[\"From the business\"].apply(lambda x: 1 if isinstance(x, list) and 'Identifies as Black-owned' in x else 0)\n",
    "metadata_fl[\"women_led\"] = metadata_fl[\"From the business\"].apply(lambda x: 1 if isinstance(x, list) and 'Identifies as women-led' in x else 0)\n",
    "metadata_fl[\"veteran_led\"] = metadata_fl[\"From the business\"].apply(lambda x: 1 if isinstance(x, list) and 'Identifies as veteran-led' in x else 0)\n",
    "\n",
    "# Eliminar la columna original\n",
    "metadata_fl.drop(columns=[\"From the business\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos `Highlights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Highlights\"\n",
    "def extract_unique_highlights(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_highlights = extract_unique_highlights(metadata_fl[\"Highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Highlights'\n",
    "def transform_highlights(df, column):\n",
    "    remove_categories = {\n",
    "        'great_dessert', 'great_tea_selection', 'great_cocktails', 'great_beer_selection', \n",
    "        'great_bar_food', 'great_wine_list', 'great_produce', 'active_military_discounts', \n",
    "        'great_coffee', 'serves_local_specialty'\n",
    "    }\n",
    "    entertainment = {'play_area', 'trivia_night', 'bar_games'}\n",
    "    live_entertainment = {'live_performances', 'karaoke', 'live_music'}\n",
    "    lgbtq_friendly = {'transgender_safespace', 'lgbtq_friendly'}\n",
    "    separate_categories = {'rooftop_seating', 'fast_service', 'sports', 'fireplace'}\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def categorize_highlights(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, (list, np.ndarray, pd.Series)):\n",
    "                normalized_values = {normalize(item) for item in value}\n",
    "                return normalized_values - remove_categories\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(categorize_highlights)\n",
    "    \n",
    "    df['entertainment'] = df[column].apply(lambda x: 1 if entertainment & x else 0)\n",
    "    df['live_entertainment'] = df[column].apply(lambda x: 1 if live_entertainment & x else 0)\n",
    "    df['lgbtq_friendly'] = df[column].apply(lambda x: 1 if lgbtq_friendly & x else 0)\n",
    "    \n",
    "    for category in separate_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_highlights(metadata_fl, 'Highlights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos `Crowd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Crowd\"\n",
    "def extract_unique_crowd(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_crowd = extract_unique_crowd(metadata_fl[\"Crowd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Crowd'\n",
    "def transform_crowd(df, column):\n",
    "    separate_categories = {'locals', 'tourists', 'college_students', 'family_friendly', 'groups'}\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def categorize_crowd(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, (list, np.ndarray, pd.Series)):\n",
    "                normalized_values = {normalize(item) for item in value}\n",
    "                return normalized_values\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(categorize_crowd)\n",
    "    \n",
    "    for category in separate_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_crowd(metadata_fl, 'Crowd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos `Amenities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Amenities\"\n",
    "def extract_unique_amenities(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_amenities = extract_unique_amenities(metadata_fl[\"Amenities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Amenities'\n",
    "def transform_amenities(df, column):\n",
    "    remove_categories = {'public_restroom', 'stadium_seating'}\n",
    "    kids_friendly = {'high_chairs', 'good_for_kids'}\n",
    "    separate_categories = {'gender_neutral_restroom', 'wi_fi', 'bar_onsite'}\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def categorize_amenities(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, (list, np.ndarray, pd.Series)):\n",
    "                normalized_values = {normalize(item) for item in value}\n",
    "                return normalized_values - remove_categories\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(categorize_amenities)\n",
    "    \n",
    "    df['kids_friendly'] = df[column].apply(lambda x: 1 if kids_friendly & x else 0)\n",
    "    \n",
    "    for category in separate_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_amenities(metadata_fl, 'Amenities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis y expansión de `Payments` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Payments\"\n",
    "def extract_unique_payments(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_payments = extract_unique_payments(metadata_fl[\"Payments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Payments'\n",
    "def transform_payments(df, column):\n",
    "    separate_categories = {'credit_cards', 'debit_cards', 'nfc_mobile_payments', 'cash_only', 'checks'}\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def categorize_payments(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, (list, np.ndarray, pd.Series)):\n",
    "                normalized_values = {normalize(item) for item in value}\n",
    "                return normalized_values\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(categorize_payments)\n",
    "    \n",
    "    for category in separate_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_payments(metadata_fl, 'Payments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos y expandimos `Recycling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer todas las categorías únicas en \"Recycling\"\n",
    "def extract_unique_recycling(series):\n",
    "    unique_categories = set()\n",
    "    \n",
    "    for value in series.dropna():  # Ignorar valores nulos\n",
    "        try:\n",
    "            value = ast.literal_eval(value) if isinstance(value, str) else value  # Convertir string a lista\n",
    "            if isinstance(value, list):  # Si es una lista, agregar los valores\n",
    "                clean_values = [x.strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\") for x in value]\n",
    "                unique_categories.update(clean_values)  # Añadir al conjunto (elimina duplicados)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass  # Ignorar errores de conversión\n",
    "    \n",
    "    return unique_categories\n",
    "\n",
    "# Obtener las categorías únicas\n",
    "unique_recycling = extract_unique_recycling(metadata_fl[\"Recycling\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar 'Recycling'\n",
    "def transform_recycling(df, column):\n",
    "    separate_categories = {'plastic_bags', 'glass_bottles'}\n",
    "    \n",
    "    def normalize(text):\n",
    "        return text.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    def categorize_recycling(value):\n",
    "        if isinstance(value, float) and np.isnan(value):\n",
    "            return set()\n",
    "        try:\n",
    "            if isinstance(value, str):\n",
    "                value = ast.literal_eval(value)\n",
    "            if isinstance(value, (list, np.ndarray, pd.Series)):\n",
    "                normalized_values = {normalize(item) for item in value}\n",
    "                return normalized_values\n",
    "        except (ValueError, SyntaxError):\n",
    "            return set()\n",
    "        return set()\n",
    "    \n",
    "    df[column] = df[column].apply(categorize_recycling)\n",
    "    \n",
    "    for category in separate_categories:\n",
    "        df[category] = df[column].apply(lambda x: 1 if category in x else 0)\n",
    "    \n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Aplicar las transformaciones\n",
    "metadata_fl = transform_recycling(metadata_fl, 'Recycling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar columnas combinando valores\n",
    "metadata_fl['outdoor_seating'] = metadata_fl[['Outside food allowed', 'outdoor_seating']].max(axis=1)\n",
    "metadata_fl['kids_friendly'] = metadata_fl[['kids', 'kids_friendly']].max(axis=1)\n",
    "metadata_fl['dessert'] = metadata_fl[['dessert', 'Dessert']].max(axis=1)\n",
    "metadata_fl['lgbtq_friendly'] = metadata_fl[['lgbtq_friendly', 'gender_neutral_restroom']].max(axis=1)\n",
    "metadata_fl['recycling'] = metadata_fl[['plastic_bags', 'glass_bottles']].max(axis=1)\n",
    "\n",
    "# Eliminar las columnas antiguas\n",
    "metadata_fl.drop(columns=['Outside food allowed', 'kids', 'Dessert', 'gender_neutral_restroom', 'plastic_bags', 'glass_bottles'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar nombres de columnas para consistencia y legibilidad\n",
    "metadata_fl.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"popular_for_\", \"\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fl.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_fl.to_csv('metadatos_fl.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
